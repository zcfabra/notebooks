{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torchvision.transforms.functional as f_tf\n",
    "data_path = \"./data/mnist.pkl\"\n",
    "\n",
    "with open(data_path, \"rb\") as f:\n",
    "    data_obj = pickle.load(f)\n",
    "\n",
    "train_data = data_obj[\"train\"]\n",
    "test_data = data_obj[\"test\"]\n",
    "train_data, test_data = torch.Tensor(train_data), torch.Tensor(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Implementation details\n",
    "# No spatial pooling, strided convs instead\n",
    "# No FCs ontop of conv features\n",
    "# BatchNorm applied to all layers except the generatr output and discriminator input\n",
    "# * Batchnorm helps prevent mode collapse, but applying to every layer caused instability (per the authors)\n",
    "# ReLUs in generator, tanh output; discriminator leaky ReLU (slope of 0.2) for all layers\n",
    "# Adam w/ lr = 0.0002, momentum reduced to 0.5\n",
    "\n",
    "# relu before batchnorm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_shape,batch_size,latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.in_shape = in_shape\n",
    "        self.nin = np.prod(self.in_shape)\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.features = in_shape[0]\n",
    "\n",
    "        c1 = nn.ConvTranspose2d(self.latent_dim,self.features * 16,(4,4), stride=1,padding=0)\n",
    "        c2 = nn.ConvTranspose2d(self.features * 16,self.features * 8,4,stride=2,padding=1)\n",
    "        c3 = nn.ConvTranspose2d(self.features * 8,self.features * 4,4,stride=2,padding=1)\n",
    "        c4 = nn.ConvTranspose2d(self.features * 4,self.features*2,4,stride=2,padding=1)\n",
    "        c5 = nn.ConvTranspose2d(self.features*2,1,4,stride=2,padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.act_out = nn.Tanh()\n",
    "\n",
    "        self.enc = nn.Sequential(c1,self.relu, nn.BatchNorm2d(self.features*16), c2, self.relu,nn.BatchNorm2d(self.features*8), c3, self.relu,nn.BatchNorm2d(self.features*4), c4, self.relu,nn.BatchNorm2d(self.features*2), c5, self.act_out)\n",
    "\n",
    "    def forward(self):\n",
    "        X = torch.randn(self.batch_size,self.latent_dim).unsqueeze(-1).unsqueeze(-1)\n",
    "        # print(X.shape)\n",
    "        return self.enc(X)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_shape, hidden_size, latent_dim):\n",
    "        super(Discriminator,self).__init__()\n",
    "\n",
    "\n",
    "        self.in_shape = in_shape\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_dim = latent_dim\n",
    "        n_f = self.hidden_size\n",
    "\n",
    "        self.c1 = nn.Conv2d(1, n_f, 4,2,1)\n",
    "        self.c2 = nn.Conv2d(n_f, n_f*2, 4,2,1)\n",
    "        self.c3 = nn.Conv2d(n_f*2, n_f*4, 4,2,1)\n",
    "        self.c4 = nn.Conv2d(n_f*4, n_f*8, 4,2,1)\n",
    "        self.c5 = nn.Conv2d(n_f*8, n_f*16, 4,2,1)\n",
    "\n",
    "        self.flatten = nn.Flatten(1,-1)\n",
    "\n",
    "        self.out = nn.Linear(n_f * 16 * 2 * 2, 1)\n",
    "        \n",
    "\n",
    "        self.disc = nn.Sequential(self.c1, nn.LeakyReLU(0.2), nn.BatchNorm2d(n_f), self.c2, nn.LeakyReLU(0.2),nn.BatchNorm2d(n_f*2), self.c3, nn.LeakyReLU(0.2),nn.BatchNorm2d(n_f*4), self.c4, nn.LeakyReLU(0.2), nn.BatchNorm2d(n_f*8),self.c5, nn.LeakyReLU(0.2),nn.BatchNorm2d(n_f*16), self.flatten, self.out, nn.Sigmoid())\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.disc(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "gen = Generator((64,64,1), batch_size, 100)\n",
    "disc = Discriminator((64,64,1), 64,100)\n",
    "num_epochs = 50\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "optim_disc = torch.optim.Adam(disc.parameters(), lr = 0.0002, betas=[0.5, 0.99])\n",
    "optim_gen = torch.optim.Adam(disc.parameters(), lr = 0.0002, betas=[0.5, 0.99])\n",
    "\n",
    "for ep in trange(num_epochs):\n",
    "    for src in loader:\n",
    "        optim_gen.zero_grad()\n",
    "        optim_disc.zero_grad()\n",
    "        src = src.permute(0,3,1,2)\n",
    "        src[src>0] = 1.\n",
    "        # print(src.shape)\n",
    "        real = f_tf.resize(src, (64))\n",
    "        fake = gen()\n",
    "        # print(fake.shape)\n",
    "        disc_real = disc(real)\n",
    "        disc_fake = disc(fake)\n",
    "        # print(disc_real.shape)\n",
    "        # print(disc_fake.shape)\n",
    "\n",
    "        disc_real_loss = loss_fn(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake_loss = loss_fn(disc_fake, torch.ones_like(disc_fake))\n",
    "        disc_loss = (disc_real_loss + disc_fake_loss) / 2\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "        optim_disc.step()\n",
    "\n",
    "        out = disc(fake)\n",
    "        gen_loss = loss_fn(out, torch.ones_like(out))\n",
    "        gen_loss.backward()\n",
    "        optim_gen.step()\n",
    "\n",
    "    torch.save({\n",
    "        \"epoch\": ep,\n",
    "        \"disc_state\": disc.state_dict(),\n",
    "        \"gen_state\": gen.state_dict(),\n",
    "        \"disc_opt\": optim_disc.state_dict(),\n",
    "        \"gen_opt\": optim_gen.state_dict(),\n",
    "        \"gen_loss\": gen_loss,\n",
    "        \"disc_loss\": disc_loss\n",
    "    }, f\"model_ep_{ep}.pt\")\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
