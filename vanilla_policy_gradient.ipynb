{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: learn right setting of weights to maximize the expected rewards\n",
    "# Want an NN that outputs distribution over actions\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, n_actions, n_states, hidden_size):\n",
    "        super(Agent, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_states, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "            nn.Softmax() # b/c we want a distribution over action space\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "\n",
    "class Val(nn.Module):\n",
    "    def __init__(self, n_states, hidden_size, out_size):\n",
    "        super (Val, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_states, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, out_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "\n",
    "\n",
    "class VPG():\n",
    "    def __init__(self,n_actions, n_states, hidden_size, buffer_size, learning_rate, batch_size, gamma):\n",
    "        self.agent = Agent(n_actions, n_states, hidden_size)\n",
    "        self.val = Val(n_states, hidden_size, 1)\n",
    "        self.gamma = gamma\n",
    "        self.states= []\n",
    "        self.rewards=[]\n",
    "        self.next_states=[]\n",
    "        self.actions = []\n",
    "        self.chosen_actions = []\n",
    "        self.episode_rewards = []\n",
    "        \n",
    "        self.optim = torch.optim.Adam(self.agent.parameters(), lr=learning_rate)\n",
    "\n",
    "    def add_observation(self, state, action, chosen_action, reward, next_state):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.chosen_actions.append(chosen_action)\n",
    "        self.rewards.append(reward)\n",
    "        self.next_states.append(next_state)\n",
    "\n",
    "    def get_avg_reward_for_episode(self):\n",
    "        reward = sum(self.rewards)\n",
    "        self.episode_rewards.append(reward)\n",
    "        return reward\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        self.optim.zero_grad()\n",
    "        rewards_to_go = np.sum([self.gamma**ix * np.array(self.rewards[ix]) for ix, each in enumerate(self.rewards)])\n",
    "        sampler = Categorical(self.agent(torch.Tensor(self.states)))\n",
    "\n",
    "        loss = -torch.sum(sampler.log_prob(torch.Tensor(self.chosen_actions)) * rewards_to_go)\n",
    "        # print(\"LOSS\",loss.shape)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "\n",
    "\n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.actions = []\n",
    "        self.chosen_actions = []\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 4\n",
    "n_states = 8\n",
    "hidden_size = 64\n",
    "buffer_size = 1000000\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "model = VPG(n_actions, n_states, hidden_size, buffer_size, learning_rate, batch_size, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "for ep in (pbar:=tqdm(range(1000))): # num. episodes\n",
    "   observation, info = env.reset(seed=42)\n",
    "   terminated, truncated = False, False\n",
    "   while not terminated and not truncated:\n",
    "      actions = model.agent(torch.Tensor(observation))\n",
    "      chosen_action = Categorical(actions).sample().item()\n",
    "      next_observation, reward, terminated, truncated, info = env.step(chosen_action)\n",
    "\n",
    "      model.add_observation(observation, actions, chosen_action, reward, next_observation)\n",
    "      observation = next_observation\n",
    "   ep_reward = model.get_avg_reward_for_episode()\n",
    "   model.update()\n",
    "   model.clear()\n",
    "\n",
    "\n",
    "   pbar.set_description(\"Ep: {} Reward: {:.3}\".format(ep, ep_reward))\n",
    "\n",
    "   \n",
    "   \n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
