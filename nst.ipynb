{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "vgg = torchvision.models.vgg19(pretrained=True).features.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =Image.open(\"./test_images/lion.jpeg\").convert(\"RGB\")\n",
    "style = Image.open(\"./test_images/style.jpeg\").convert(\"RGB\")\n",
    "\n",
    "shape = 224\n",
    "transf = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    # torchvision.transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]),\n",
    "    torchvision.transforms.Resize((shape,shape))\n",
    "])\n",
    "\n",
    "inp = transf(test).to(device)\n",
    "plt.imshow(inp.cpu().permute(1,2,0).numpy())\n",
    "inp = inp.unsqueeze(0)\n",
    "style = transf(style).to(device)\n",
    "print(style.shape)\n",
    "plt.imshow(style.permute(1,2,0).cpu().numpy())\n",
    "style = style.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.max(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self,model, key_features=['0', '5', '10', '19', '28']):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.key_features = key_features\n",
    "        self.model = model\n",
    "        for layer in self.model:\n",
    "            layer.requires_grad_(False)\n",
    "    def forward(self, x):\n",
    "        out =[]\n",
    "        for ix, layer in enumerate(self.model):\n",
    "            x = layer(x)\n",
    "            if str(ix) in self.key_features:\n",
    "                out.append(x)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grams(x):\n",
    "    # (b,ch,h,w)= x.size()\n",
    "    x = x.view(x.shape[0], x.shape[1], -1)\n",
    "    # print(x.shape)\n",
    "    x_t = x.transpose(1,2)\n",
    "    # print(x_t.shape)\n",
    "    o = x.bmm(x_t)\n",
    "    # print(o.shape)\n",
    "    return o\n",
    "    # return o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen = torch.randn((1,3,shape,shape), requires_grad=True, device=device)\n",
    "gen = inp.clone().requires_grad_(True).to(device)\n",
    "optim = torch.optim.Adam([gen], lr=0.01)\n",
    "features = FeatureExtractor(vgg).to(device)\n",
    "features.eval()\n",
    "losses = []\n",
    "features_extracted_source = features(inp)\n",
    "features_extracted_style = features(style)\n",
    "style_weight, content_weight = 0.1, 1.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for _ in trange(1000):\n",
    "    content_loss = style_loss = 0.0\n",
    "    optim.zero_grad()\n",
    "    features_extracted_generated = features(gen)\n",
    "    for source_features, gen_features, style_features in zip(features_extracted_source,features_extracted_generated, features_extracted_style):\n",
    "        # print(source_features.shape, gen_features.shape)\n",
    "        source_features = source_features.view(source_features.shape[0], source_features.shape[1], -1)\n",
    "        gen_features= gen_features.view(gen_features.shape[0], gen_features.shape[1], -1)\n",
    "        # print(source_features.shape, gen_features.shape)\n",
    "\n",
    "        style_grams = grams(style_features)\n",
    "        gen_grams = grams(gen_features)\n",
    "        # print(style_grams[0].shape, gen_grams[0].shape)\n",
    "\n",
    "        # print(source_features.shape, gen_features.shape)\n",
    "        content_loss += torch.mean((source_features[0]- gen_features[0])**2)\n",
    "        style_loss += torch.mean((style_grams[0] - gen_grams[0])**2)\n",
    "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "    losses.append(total_loss.item())\n",
    "    total_loss.backward()\n",
    "\n",
    "    optim.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(gen.permute(1,2,0).cpu().detach().numpy())\n",
    "print(torch.max(gen))\n",
    "img = torchvision.transforms.functional.to_pil_image(gen.squeeze(0))\n",
    "plt.imshow(img)\n",
    "# img.save(\"./test_images/out_3000_0_1.jpg\")\n",
    "# plt.imshow((gen.squeeze(0).permute(1,2,0)).cpu().detach().numpy())\n",
    "print(losses[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
