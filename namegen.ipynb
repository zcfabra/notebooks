{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/doriclink/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  ...,  8,  8,  8],\n",
      "        [ 2,  3,  9,  ...,  8,  8,  8],\n",
      "        [14,  5, 10,  ...,  8,  8,  8],\n",
      "        ...,\n",
      "        [ 4,  4,  9,  ...,  8,  8,  8],\n",
      "        [ 4,  4,  0,  ...,  8,  8,  8],\n",
      "        [ 4,  4,  0,  ...,  8,  8,  8]], dtype=torch.uint8)\n",
      "tensor([[ 1,  2,  3,  ...,  8,  8,  8],\n",
      "        [ 3,  9,  1,  ...,  8,  8,  8],\n",
      "        [ 5, 10, 10,  ...,  8,  8,  8],\n",
      "        ...,\n",
      "        [ 4,  9,  1,  ...,  8,  8,  8],\n",
      "        [ 4,  0,  1,  ...,  8,  8,  8],\n",
      "        [ 4,  0,  4,  ...,  8,  8,  8]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/names.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "names = [each.lower().strip() for each in data]\n",
    "\n",
    "names_padded = []\n",
    "\n",
    "name_lens = [len(each) for each in names]\n",
    "\n",
    "max_len = max(name_lens)\n",
    "\n",
    "for name in names:\n",
    "    names_padded.append(list(name) + [\"<EOF>\"] + [\"*\"] * (max_len - len(name)))\n",
    "\n",
    "vocab = {}\n",
    "\n",
    "idx=0\n",
    "\n",
    "for name in names_padded:\n",
    "    for char in name:\n",
    "        if char not in vocab:\n",
    "            vocab[char] = idx\n",
    "            idx+=1\n",
    "\n",
    "targets = [each[1:] +[\"*\"] for each in names_padded]\n",
    "\n",
    "\n",
    "tensors_from_names = torch.Tensor([[vocab[char] for char in name] for name in names_padded]).type(torch.uint8)\n",
    "tensors_from_targets = torch.Tensor([[vocab[char] for char in name] for name in targets]).type(torch.uint8)\n",
    "\n",
    "print(tensors_from_names)\n",
    "print(tensors_from_targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.emb = nn.Embedding(len(vocab), self.hidden_size, padding_idx=vocab[\"*\"])\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, num_layers=self.num_layers, bias=True, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_size, len(vocab))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X, h):\n",
    "        X = self.emb(X)\n",
    "        X = self.relu(X)\n",
    "\n",
    "        X, h = self.gru(X, h)\n",
    "        X = self.relu(X)\n",
    "\n",
    "        X = self.fc(X)\n",
    "        return X, h\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        return torch.zeros(self.num_layers, bs, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, src, tgt):\n",
    "        self.src = src\n",
    "        self.tgt = tgt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src[idx], self.tgt[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\") if (torch.backends.mps.is_available() and torch.backends.mps.is_built() ) else \"cpu\"\n",
    "print(device)\n",
    "batch_size = 128\n",
    "dataset =  CustomDataset(tensors_from_names, tensors_from_targets)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "gen = Generator(256, 2, vocab)\n",
    "gen = gen.to(device)\n",
    "\n",
    "optim = torch.optim.Adam(gen.parameters(), lr=3e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(10):\n",
    "    h = gen.init_hidden(batch_size).to(device)\n",
    "    for src, tgt in tqdm(loader):\n",
    "        if src.shape[0] != batch_size:\n",
    "            continue\n",
    "        src = src.to(device).int()\n",
    "        tgt = tgt.to(device).int()\n",
    "        preds, h = gen(src, h)\n",
    "        h = h.detach()\n",
    "        \n",
    "\n",
    "        loss = loss_fn(preds.permute(0,2,1), tgt)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
