{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from tqdm import tqdm, trange\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/names.txt\"\n",
    "\n",
    "with open(data_path, \"r\") as f:\n",
    "    raw_names = f.readlines()\n",
    "\n",
    "names_processed  = [name.lower().strip() for name in raw_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(list_of_names):\n",
    "    vocab = {}\n",
    "    idx = 0\n",
    "    for each in list_of_names:\n",
    "        for char in each:\n",
    "            if char not in vocab:\n",
    "                vocab[char] = idx\n",
    "                idx+=1\n",
    "    vocab[\"*\"] = idx\n",
    "    idx+=1\n",
    "    vocab[\"<EOF>\"] = idx\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(list_of_names):\n",
    "    max_len = max([len(name) for name in list_of_names])\n",
    "    out = []\n",
    "\n",
    "    for name in list_of_names:\n",
    "        req = max_len - len(name)\n",
    "        chars = list(name)\n",
    "        chars += [\"<EOF>\"] + [\"*\"] * req\n",
    "        out.append(chars)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_torch(list_of_names, vocab):\n",
    "    out = []\n",
    "    for name in list_of_names:\n",
    "        chars = [vocab[char] for char in name]\n",
    "        out.append(chars)\n",
    "    \n",
    "    return torch.Tensor((out))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab(names_processed)\n",
    "# print(vocab)\n",
    "padded_names = pad(names_processed)\n",
    "padded_tgt = []\n",
    "for name in padded_names:\n",
    "    padded_tgt.append(name[1:] + [\"*\"])\n",
    "input_data = list_to_torch(padded_names, vocab)\n",
    "input_targets = list_to_torch(padded_tgt, vocab)\n",
    "\n",
    "\n",
    "class CustomSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, tgt, src):\n",
    "        self.tgt = tgt\n",
    "        self.src = src\n",
    "        self.transform = None\n",
    "        self.target_transform = None\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src[idx], self.tgt[idx]\n",
    "\n",
    "dset = CustomSet(input_data, input_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gen(nn.Module):\n",
    "    def __init__(self, num_layers, vocab, emb_size, hidden_size):\n",
    "        super(Gen, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab = vocab\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb = nn.Embedding(len(self.vocab), self.emb_size, padding_idx=self.vocab[\"*\"])\n",
    "        self.gru = nn.GRU(self.emb_size, self.hidden_size, num_layers=self.num_layers, batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(self.hidden_size, len(self.vocab))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X, h):\n",
    "        X = self.emb(X)\n",
    "        X, h = self.gru(X, h)\n",
    "        X = self.relu(X)\n",
    "        X = self.fc(X)\n",
    "        \n",
    "\n",
    "        return X, h\n",
    "    def init_hidden(self,b):\n",
    "        return torch.zeros(self.num_layers,b, self.hidden_size)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "143it [00:40,  3.57it/s]0:00<?, ?it/s]\n",
      "143it [00:49,  2.88it/s]0:40<06:00, 40.01s/it]\n",
      "143it [00:58,  2.47it/s]1:29<06:05, 45.66s/it]\n",
      "143it [00:55,  2.58it/s]2:27<05:59, 51.31s/it]\n",
      "143it [01:09,  2.05it/s]3:23<05:17, 52.97s/it]\n",
      "143it [01:04,  2.21it/s]4:32<04:55, 59.04s/it]\n",
      "143it [00:59,  2.42it/s]5:37<04:03, 60.98s/it]\n",
      "143it [01:10,  2.01it/s]6:36<03:01, 60.38s/it]\n",
      "143it [01:00,  2.37it/s]7:47<02:07, 63.76s/it]\n",
      "143it [01:02,  2.28it/s]8:48<01:02, 62.72s/it]\n",
      "100%|██████████| 10/10 [09:50<00:00, 59.10s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "model = Gen(2, vocab, 200, 300)\n",
    "optim = torch.optim.Adam(model.parameters(), lr = 0.003)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "batch_size = 128\n",
    "losses = []\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dset, batch_size=128, shuffle=True)\n",
    "for ep in trange(num_epochs):\n",
    "    h  = model.init_hidden(128)\n",
    "    for ix, (src, tgt) in tqdm(enumerate(loader)):\n",
    "        optim.zero_grad()\n",
    "        if src.shape[0] != batch_size:\n",
    "            continue\n",
    "        # print(src.shape, tgt.shape)\n",
    "        preds, h = model(src.long(), h)\n",
    "        h = h.detach()\n",
    "        # print(preds.shape)\n",
    "        preds=preds.permute(0,2,1)\n",
    "        loss = loss_fn(preds,tgt.long())\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"weights.pth\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n",
      "torch.Size([2, 1, 300])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "m\n",
      "a\n",
      "m\n",
      "a\n",
      "m\n",
      "a\n",
      "m\n",
      "a\n",
      "m\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "starting = \"a\"\n",
    "length = 10\n",
    "model.load_state_dict(torch.load(\"weights.pth\"))\n",
    "x = torch.tensor(vocab[starting])\n",
    "x = x.unsqueeze(0).unsqueeze(0)\n",
    "print(x.shape)\n",
    "word = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    h = model.init_hidden(1)\n",
    "    print(h.shape)\n",
    "    for each in range(length):\n",
    "        pred, h = model(x, h)\n",
    "    \n",
    "        # print(pred.shape)\n",
    "        pred = pred.softmax(dim=2).squeeze(1)\n",
    "        # print(pred.shape)\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        print(pred.shape)\n",
    "        x = pred.unsqueeze(1)\n",
    "        word.append(pred.item())\n",
    "for each in word:\n",
    "    print(list(vocab.keys())[list(vocab.values()).index(each)])  # Prints george\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
